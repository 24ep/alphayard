# =============================================
# PostgreSQL Optimized Configuration
# For 100k-1M users scale
# =============================================

# ---------------------------------------------
# CONNECTIONS
# ---------------------------------------------
# Increase for high concurrency (use PgBouncer for 500+ connections)
max_connections = 200

# ---------------------------------------------
# MEMORY SETTINGS
# Adjust based on available server RAM
# ---------------------------------------------

# Shared buffers: 25% of total RAM (example: 2GB for 8GB server)
shared_buffers = 2GB

# Effective cache size: 50-75% of RAM (for query planner)
effective_cache_size = 6GB

# Work memory for sorting/hashing per query (careful with concurrent queries)
work_mem = 32MB

# Memory for maintenance operations (VACUUM, CREATE INDEX)
maintenance_work_mem = 512MB

# Huge pages (enable if OS supports it for large shared_buffers)
# huge_pages = try

# ---------------------------------------------
# WRITE-AHEAD LOGGING (WAL)
# ---------------------------------------------
# WAL level for replication support
wal_level = replica

# Maximum WAL size before checkpoint
max_wal_size = 2GB
min_wal_size = 256MB

# Checkpoint settings
checkpoint_completion_target = 0.9
checkpoint_timeout = 10min

# WAL buffers (auto-tuned but can set explicitly)
wal_buffers = 64MB

# ---------------------------------------------
# QUERY PLANNER
# ---------------------------------------------
# For SSD storage (lower is better for SSD)
random_page_cost = 1.1

# Parallel query settings
max_parallel_workers_per_gather = 4
max_parallel_workers = 8
max_parallel_maintenance_workers = 4

# Enable JIT compilation for complex queries
jit = on

# ---------------------------------------------
# STATISTICS & MONITORING
# ---------------------------------------------
# Track execution time of all statements
track_activity_query_size = 4096

# Log slow queries (adjust threshold as needed)
log_min_duration_statement = 1000  # Log queries > 1 second

# Log connection attempts
log_connections = on
log_disconnections = on

# Log lock waits
log_lock_waits = on

# Dead tuple threshold for autovacuum
autovacuum_vacuum_threshold = 50
autovacuum_analyze_threshold = 50
autovacuum_vacuum_scale_factor = 0.1
autovacuum_analyze_scale_factor = 0.05

# Increase autovacuum workers for large databases
autovacuum_max_workers = 4

# ---------------------------------------------
# REPLICATION (if using read replicas)
# ---------------------------------------------
# max_wal_senders = 3
# wal_keep_size = 1GB
# hot_standby = on

# ---------------------------------------------
# EXTENSIONS
# ---------------------------------------------
# Load pg_stat_statements for query monitoring
shared_preload_libraries = 'pg_stat_statements'

# pg_stat_statements settings
pg_stat_statements.max = 10000
pg_stat_statements.track = all
pg_stat_statements.track_utility = on

# ---------------------------------------------
# NOTES FOR PRODUCTION
# ---------------------------------------------
# 
# 1. Test changes in staging before production
# 2. Monitor with pg_stat_statements:
#    SELECT * FROM pg_stat_statements ORDER BY total_exec_time DESC LIMIT 20;
# 3. Monitor with pg_stat_user_tables:
#    SELECT relname, n_live_tup, n_dead_tup FROM pg_stat_user_tables;
# 4. Check index usage:
#    SELECT * FROM pg_stat_user_indexes ORDER BY idx_scan ASC;
# 5. Consider connection pooling (PgBouncer) for >200 concurrent connections
# 6. Set up monitoring: pgBadger, pg_stat_monitor, or Datadog/New Relic
# =============================================
